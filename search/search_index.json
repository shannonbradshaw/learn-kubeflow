{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Welcome to Arrikto Academy! Courses Kale : Transform Jupyter Notebooks into Kubeflow Pipelines","title":"Home"},{"location":"#home","text":"","title":"Home"},{"location":"#welcome-to-arrikto-academy","text":"","title":"Welcome to Arrikto Academy!"},{"location":"#courses","text":"Kale : Transform Jupyter Notebooks into Kubeflow Pipelines","title":"Courses"},{"location":"modules/deploy/minikf-gcp/","text":"Set Up MiniKF on Google Cloud Two Options You have two options for setting up MiniKF: Follow the setup video Follow the step-by-step instructions. Follow the Set Up Video Follow the Step-by-Step Instructions 1. Find MiniKF in the Google Cloud Marketplace. Open Google Cloud Marketplace and search for \"MiniKF\". 2. Select the MiniKF virtual machine by Arrikto. 3. Click the LAUNCH button and select your project. 4. Choose a name and zone for your MiniKF instance. In the Configure & Deploy window, choose a name and a zone for your MiniKF instance and leave the default options. Then click on the Deploy button. 5. Wait for the MiniKF compute instance to boot up. 6. SSH to MiniKF When the MiniKF VM is up, connect and log in by clicking on the SSH button. Follow the on-screen instructions to run the command minikf to see the progress of the deployment of Minikube, Kubeflow, and Rok. This will take a few minutes to complete. 7. Log in to MiniKF When installation is complete and all pods are ready, visit the MiniKF dashboard and log in using the MiniKF username and password: Congratulations! You have successfully deployed MiniKF on GCP. You can now create notebooks, write your ML code, run Kubeflow Pipelines, and use Rok for data versioning and reproducibility.","title":"Set Up MiniKF on Google Cloud"},{"location":"modules/deploy/minikf-gcp/#set-up-minikf-on-google-cloud","text":"","title":"Set Up MiniKF on Google Cloud"},{"location":"modules/deploy/minikf-gcp/#two-options","text":"You have two options for setting up MiniKF: Follow the setup video Follow the step-by-step instructions.","title":"Two Options"},{"location":"modules/deploy/minikf-gcp/#follow-the-set-up-video","text":"","title":"Follow the Set Up Video"},{"location":"modules/deploy/minikf-gcp/#follow-the-step-by-step-instructions","text":"","title":"Follow the Step-by-Step Instructions"},{"location":"modules/deploy/minikf-gcp/#1-find-minikf-in-the-google-cloud-marketplace","text":"Open Google Cloud Marketplace and search for \"MiniKF\".","title":"1. Find MiniKF in the Google Cloud Marketplace."},{"location":"modules/deploy/minikf-gcp/#2-select-the-minikf-virtual-machine-by-arrikto","text":"","title":"2. Select the MiniKF virtual machine by Arrikto."},{"location":"modules/deploy/minikf-gcp/#3-click-the-launch-button-and-select-your-project","text":"","title":"3. Click the LAUNCH button and select your project."},{"location":"modules/deploy/minikf-gcp/#4-choose-a-name-and-zone-for-your-minikf-instance","text":"In the Configure & Deploy window, choose a name and a zone for your MiniKF instance and leave the default options. Then click on the Deploy button.","title":"4. Choose a name and zone for your MiniKF instance."},{"location":"modules/deploy/minikf-gcp/#5-wait-for-the-minikf-compute-instance-to-boot-up","text":"","title":"5. Wait for the MiniKF compute instance to boot up."},{"location":"modules/deploy/minikf-gcp/#6-ssh-to-minikf","text":"When the MiniKF VM is up, connect and log in by clicking on the SSH button. Follow the on-screen instructions to run the command minikf to see the progress of the deployment of Minikube, Kubeflow, and Rok. This will take a few minutes to complete.","title":"6. SSH to MiniKF"},{"location":"modules/deploy/minikf-gcp/#7-log-in-to-minikf","text":"When installation is complete and all pods are ready, visit the MiniKF dashboard and log in using the MiniKF username and password: Congratulations! You have successfully deployed MiniKF on GCP. You can now create notebooks, write your ML code, run Kubeflow Pipelines, and use Rok for data versioning and reproducibility.","title":"7. Log in to MiniKF"},{"location":"modules/notebook-servers/notebook-server-base-image/","text":"Launch a Notebook Server from a Base Image To work with a notebook in Kubeflow we first need to launch a notebook server. To do this, follow the steps below. 1. View the Home screen in your MiniKF Kubeflow deployment. 2. Select the Notebooks pane from the main navigation menu. You will see the Notebook Servers dashboard. Unless you have already launched one or more notebook servers, your dashboard will be empty. 3. Click the NEW SERVER button. Once you have clicked NEW SERVER, a form will appear that will enable you to provide the settings for your notebook server. 4. Enter a name. In the Name field, enter a name, e.g., learn-kubeflow-pipelines. 5. Add a data volume. Scroll down to the Data Volumes field. Add a data volume. The field values will auto-complete. You do not need to make any changes to the field values set for you. 6. Click the LAUNCH button. Scroll to the bottom of the form and click the LAUNCH button to create your notebook server. 7. Connect to your notebook server. To connect to the notebook server you\u2019ve just created, click the CONNECT button in the Notebook Servers dashboard. Once you connect, you will see the Jupyter notebooks server user interface with an open file browser pane and launcher tab.","title":"Deploy a Notebook Server"},{"location":"modules/notebook-servers/notebook-server-base-image/#launch-a-notebook-server-from-a-base-image","text":"To work with a notebook in Kubeflow we first need to launch a notebook server. To do this, follow the steps below.","title":"Launch a Notebook Server from a Base Image"},{"location":"modules/notebook-servers/notebook-server-base-image/#1-view-the-home-screen-in-your-minikf-kubeflow-deployment","text":"","title":"1. View the Home screen in your MiniKF Kubeflow deployment."},{"location":"modules/notebook-servers/notebook-server-base-image/#2-select-the-notebooks-pane-from-the-main-navigation-menu","text":"You will see the Notebook Servers dashboard. Unless you have already launched one or more notebook servers, your dashboard will be empty.","title":"2. Select the Notebooks pane from the main navigation menu."},{"location":"modules/notebook-servers/notebook-server-base-image/#3-click-the-new-server-button","text":"Once you have clicked NEW SERVER, a form will appear that will enable you to provide the settings for your notebook server.","title":"3. Click the NEW SERVER button."},{"location":"modules/notebook-servers/notebook-server-base-image/#4-enter-a-name","text":"In the Name field, enter a name, e.g., learn-kubeflow-pipelines.","title":"4. Enter a name."},{"location":"modules/notebook-servers/notebook-server-base-image/#5-add-a-data-volume","text":"Scroll down to the Data Volumes field. Add a data volume. The field values will auto-complete. You do not need to make any changes to the field values set for you.","title":"5. Add a data volume."},{"location":"modules/notebook-servers/notebook-server-base-image/#6-click-the-launch-button","text":"Scroll to the bottom of the form and click the LAUNCH button to create your notebook server.","title":"6. Click the LAUNCH button."},{"location":"modules/notebook-servers/notebook-server-base-image/#7-connect-to-your-notebook-server","text":"To connect to the notebook server you\u2019ve just created, click the CONNECT button in the Notebook Servers dashboard. Once you connect, you will see the Jupyter notebooks server user interface with an open file browser pane and launcher tab.","title":"7. Connect to your notebook server."},{"location":"modules/notebook-to-pipeline/","text":"Kale 101 : Transform Jupyter Notebooks into Kubeflow Pipelines Course Summary When first learning to use Kubeflow, you will probably want to adapt one or more existing Python scripts or Jupyter notebooks so that they can be deployed as a Kubeflow pipeline. In this module we will prepare you to define Kubeflow pipelines based on existing code or from scratch as you develop new models. Because it\u2019s the simplest way to get started, we will use MiniKF as our Kubeflow environment. We\u2019ll teach you how to organize and annotate cells in a Jupyter Notebook to define a Kubeflow pipeline that will run on a Kubernetes cluster. This does not require any specialized knowledge of Kubernetes. Instead, we\u2019ll use the open-source Kale JupyterLab extension.","title":"Home"},{"location":"modules/notebook-to-pipeline/#kale-101-transform-jupyter-notebooks-into-kubeflow-pipelines","text":"","title":"Kale 101: Transform Jupyter Notebooks into Kubeflow Pipelines"},{"location":"modules/notebook-to-pipeline/#course-summary","text":"When first learning to use Kubeflow, you will probably want to adapt one or more existing Python scripts or Jupyter notebooks so that they can be deployed as a Kubeflow pipeline. In this module we will prepare you to define Kubeflow pipelines based on existing code or from scratch as you develop new models. Because it\u2019s the simplest way to get started, we will use MiniKF as our Kubeflow environment. We\u2019ll teach you how to organize and annotate cells in a Jupyter Notebook to define a Kubeflow pipeline that will run on a Kubernetes cluster. This does not require any specialized knowledge of Kubernetes. Instead, we\u2019ll use the open-source Kale JupyterLab extension.","title":"Course Summary"},{"location":"modules/notebook-to-pipeline/checkpoint-fundamentals/","text":"Checkpoint: Kale Fundamentals At this point, we\u2019ve covered the Pipeline Step , Imports , and Skip Cell annotations. With a little review here and there you should now be able to recall how to do the following: Identify code in a notebook that implements a discrete step in a machine learning workflow and annotate that cell as a Pipeline Step. Identify the data that one step produces as output and the step or steps that depend on that data as input. Specify dependency relationships between Kubeflow pipeline steps using the Depends on parameter of the Pipeline Step annotation. Organize the Python statements that import modules your pipeline steps need into a small number of cells and mark those cells using the Imports annotation. Identify cells in a notebook that should be excluded from pipeline runs and annotate them as Skip Cells.","title":"Checkpoint: Kale Fundamentals"},{"location":"modules/notebook-to-pipeline/checkpoint-fundamentals/#checkpoint-kale-fundamentals","text":"At this point, we\u2019ve covered the Pipeline Step , Imports , and Skip Cell annotations. With a little review here and there you should now be able to recall how to do the following: Identify code in a notebook that implements a discrete step in a machine learning workflow and annotate that cell as a Pipeline Step. Identify the data that one step produces as output and the step or steps that depend on that data as input. Specify dependency relationships between Kubeflow pipeline steps using the Depends on parameter of the Pipeline Step annotation. Organize the Python statements that import modules your pipeline steps need into a small number of cells and mark those cells using the Imports annotation. Identify cells in a notebook that should be excluded from pipeline runs and annotate them as Skip Cells.","title":"Checkpoint: Kale Fundamentals"},{"location":"modules/notebook-to-pipeline/introduction/","text":"Course Overview","title":"Course Overview"},{"location":"modules/notebook-to-pipeline/introduction/#course-overview","text":"","title":"Course Overview"},{"location":"modules/notebook-to-pipeline/minikf-gcp/","text":"Set Up MiniKF on Google Cloud Two Options You have two options for setting up MiniKF: Follow the setup video Follow the step-by-step instructions. Follow the Set Up Video Follow the Step-by-Step Instructions 1. Find MiniKF in the Google Cloud Marketplace. Open Google Cloud Marketplace and search for \"MiniKF\". 2. Select the MiniKF virtual machine by Arrikto. 3. Click the LAUNCH button and select your project. 4. Choose a name and zone for your MiniKF instance. In the Configure & Deploy window, choose a name and a zone for your MiniKF instance and leave the default options. Then click on the Deploy button. 5. Wait for the MiniKF compute instance to boot up. 6. SSH to MiniKF When the MiniKF VM is up, connect and log in by clicking on the SSH button. Follow the on-screen instructions to run the command minikf to see the progress of the deployment of Minikube, Kubeflow, and Rok. This will take a few minutes to complete. 7. Log in to MiniKF When installation is complete and all pods are ready, visit the MiniKF dashboard and log in using the MiniKF username and password: Congratulations! You have successfully deployed MiniKF on GCP. You can now create notebooks, write your ML code, run Kubeflow Pipelines, and use Rok for data versioning and reproducibility.","title":"Deploy MiniKF on GCP"},{"location":"modules/notebook-to-pipeline/minikf-gcp/#set-up-minikf-on-google-cloud","text":"","title":"Set Up MiniKF on Google Cloud"},{"location":"modules/notebook-to-pipeline/minikf-gcp/#two-options","text":"You have two options for setting up MiniKF: Follow the setup video Follow the step-by-step instructions.","title":"Two Options"},{"location":"modules/notebook-to-pipeline/minikf-gcp/#follow-the-set-up-video","text":"","title":"Follow the Set Up Video"},{"location":"modules/notebook-to-pipeline/minikf-gcp/#follow-the-step-by-step-instructions","text":"","title":"Follow the Step-by-Step Instructions"},{"location":"modules/notebook-to-pipeline/minikf-gcp/#1-find-minikf-in-the-google-cloud-marketplace","text":"Open Google Cloud Marketplace and search for \"MiniKF\".","title":"1. Find MiniKF in the Google Cloud Marketplace."},{"location":"modules/notebook-to-pipeline/minikf-gcp/#2-select-the-minikf-virtual-machine-by-arrikto","text":"","title":"2. Select the MiniKF virtual machine by Arrikto."},{"location":"modules/notebook-to-pipeline/minikf-gcp/#3-click-the-launch-button-and-select-your-project","text":"","title":"3. Click the LAUNCH button and select your project."},{"location":"modules/notebook-to-pipeline/minikf-gcp/#4-choose-a-name-and-zone-for-your-minikf-instance","text":"In the Configure & Deploy window, choose a name and a zone for your MiniKF instance and leave the default options. Then click on the Deploy button.","title":"4. Choose a name and zone for your MiniKF instance."},{"location":"modules/notebook-to-pipeline/minikf-gcp/#5-wait-for-the-minikf-compute-instance-to-boot-up","text":"","title":"5. Wait for the MiniKF compute instance to boot up."},{"location":"modules/notebook-to-pipeline/minikf-gcp/#6-ssh-to-minikf","text":"When the MiniKF VM is up, connect and log in by clicking on the SSH button. Follow the on-screen instructions to run the command minikf to see the progress of the deployment of Minikube, Kubeflow, and Rok. This will take a few minutes to complete.","title":"6. SSH to MiniKF"},{"location":"modules/notebook-to-pipeline/minikf-gcp/#7-log-in-to-minikf","text":"When installation is complete and all pods are ready, visit the MiniKF dashboard and log in using the MiniKF username and password: Congratulations! You have successfully deployed MiniKF on GCP. You can now create notebooks, write your ML code, run Kubeflow Pipelines, and use Rok for data versioning and reproducibility.","title":"7. Log in to MiniKF"},{"location":"modules/notebook-to-pipeline/upload-handouts/","text":"Get Dataset and Code To work through this module you will need the code and data we have provided. Please download and unzip the handout . Upload the handout files Once you\u2019ve unzipped the handout, you should see the following files. 1. Review the handout files car_prices.csv is our data file. data_dictionary-carprices.xlsx provides some explanatory detail on our dataset. predict_car_price.ipynb is a notebook containing Python code that builds and evaluates three models for predicting car prices based on our dataset. We will modify the code in small ways and annotate this notebook to define and run a Kubeflow pipeline! requirements.txt lists the Python modules required for our notebook. We'll use this file to install those requirements in a later step. 2. Open the learn-kubeflow-pipelines-vol-1 folder Double-click on the directory, learn-kubeflow-pipelines-vol-1 . 3. Click the file upload button 4. Upload handout files In the file dialog that pops up, select the three handout files you unzipped and upload them to your Jupyter notebook environment. You will see them appear in the learn-kubeflow-pipelines-vol-1 directory. 5. Create a new folder Click the button to create a new folder. 6. Name the folder \"data\" 7. Move data files Drag and drop car_prices.csv and data_dictionary-carprices.xlsx into the data folder. 8. Open our notebook Double-click predict_car_price.ipynb in the file browser pane. 9. Enable Kale Click the Enable toggle in the Kale Deployment panel to enable Kale. 10. Launch a Terminal Click the Launcher tab and launch a terminal. 11. Install Requirements In the terminal enter the following commands. Change to the learn-kubeflow-pipelines-vol-1 directory. cd learn-kubeflow-pipelines-vol-1 Install the Python modules required by this notebook. pip install -r requirements.txt 12. Restart the Kernel In the predict_car_price.ipynb notebook, restart the kernel.","title":"Get Dataset and Code"},{"location":"modules/notebook-to-pipeline/upload-handouts/#get-dataset-and-code","text":"To work through this module you will need the code and data we have provided. Please download and unzip the handout .","title":"Get Dataset and Code"},{"location":"modules/notebook-to-pipeline/upload-handouts/#upload-the-handout-files","text":"Once you\u2019ve unzipped the handout, you should see the following files.","title":"Upload the handout files"},{"location":"modules/notebook-to-pipeline/upload-handouts/#1-review-the-handout-files","text":"car_prices.csv is our data file. data_dictionary-carprices.xlsx provides some explanatory detail on our dataset. predict_car_price.ipynb is a notebook containing Python code that builds and evaluates three models for predicting car prices based on our dataset. We will modify the code in small ways and annotate this notebook to define and run a Kubeflow pipeline! requirements.txt lists the Python modules required for our notebook. We'll use this file to install those requirements in a later step.","title":"1. Review the handout files"},{"location":"modules/notebook-to-pipeline/upload-handouts/#2-open-the-learn-kubeflow-pipelines-vol-1-folder","text":"Double-click on the directory, learn-kubeflow-pipelines-vol-1 .","title":"2. Open the learn-kubeflow-pipelines-vol-1 folder"},{"location":"modules/notebook-to-pipeline/upload-handouts/#3-click-the-file-upload-button","text":"","title":"3. Click the file upload button"},{"location":"modules/notebook-to-pipeline/upload-handouts/#4-upload-handout-files","text":"In the file dialog that pops up, select the three handout files you unzipped and upload them to your Jupyter notebook environment. You will see them appear in the learn-kubeflow-pipelines-vol-1 directory.","title":"4. Upload handout files"},{"location":"modules/notebook-to-pipeline/upload-handouts/#5-create-a-new-folder","text":"Click the button to create a new folder.","title":"5. Create a new folder"},{"location":"modules/notebook-to-pipeline/upload-handouts/#6-name-the-folder-data","text":"","title":"6. Name the folder \"data\""},{"location":"modules/notebook-to-pipeline/upload-handouts/#7-move-data-files","text":"Drag and drop car_prices.csv and data_dictionary-carprices.xlsx into the data folder.","title":"7. Move data files"},{"location":"modules/notebook-to-pipeline/upload-handouts/#8-open-our-notebook","text":"Double-click predict_car_price.ipynb in the file browser pane.","title":"8. Open our notebook"},{"location":"modules/notebook-to-pipeline/upload-handouts/#9-enable-kale","text":"Click the Enable toggle in the Kale Deployment panel to enable Kale.","title":"9. Enable Kale"},{"location":"modules/notebook-to-pipeline/upload-handouts/#10-launch-a-terminal","text":"Click the Launcher tab and launch a terminal.","title":"10. Launch a Terminal"},{"location":"modules/notebook-to-pipeline/upload-handouts/#11-install-requirements","text":"In the terminal enter the following commands. Change to the learn-kubeflow-pipelines-vol-1 directory. cd learn-kubeflow-pipelines-vol-1 Install the Python modules required by this notebook. pip install -r requirements.txt","title":"11. Install Requirements"},{"location":"modules/notebook-to-pipeline/upload-handouts/#12-restart-the-kernel","text":"In the predict_car_price.ipynb notebook, restart the kernel.","title":"12. Restart the Kernel"},{"location":"modules/notebook-to-pipeline/working-example/","text":"Notebook Walkthrough","title":"Notebook Walkthrough"},{"location":"modules/notebook-to-pipeline/working-example/#notebook-walkthrough","text":"","title":"Notebook Walkthrough"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-cells-solution/","text":"Solution - Lab: Skip Cells Requirements Update the cells in the Clean Data section of our notebook so that cells not necessary for cleaning data will be excluded from the clean_data pipeline step. Here\u2019s what clean_data looks like as you begin this exercise. Solution We definitely need the first cell in the Clean Data section so no changes are required here. This step, however, displays the data types for fields in this data set. We use the output to identify the fact that we need to change the data type for the \u2018symboling\u2019 column to \u2018str\u2019. However, we do not need to execute the cell on each run of the pipeline we\u2019re building. The first two cells in the block below work together to fix some misspellings and ensure that we are using the same label for \u2018CarName\u2019 across all rows. The cell below is diagnostic. It turns out that we don\u2019t have any duplicates in this data set, but even if we did, we do not need to run this cell as part of the clean_data step or any pipeline step. We would only run cells that dealt with duplicates. This cell is just a means of getting a quick peek at the data. It doesn\u2019t do any data cleaning. !!! important \"Follow Along Before continuing, please ensure the clean_data step in your notebook matches the solution above.","title":"Solution - Lab: Skip Cells"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-cells-solution/#solution-lab-skip-cells","text":"","title":"Solution - Lab: Skip Cells"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-cells-solution/#requirements","text":"Update the cells in the Clean Data section of our notebook so that cells not necessary for cleaning data will be excluded from the clean_data pipeline step. Here\u2019s what clean_data looks like as you begin this exercise.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-cells-solution/#solution","text":"We definitely need the first cell in the Clean Data section so no changes are required here. This step, however, displays the data types for fields in this data set. We use the output to identify the fact that we need to change the data type for the \u2018symboling\u2019 column to \u2018str\u2019. However, we do not need to execute the cell on each run of the pipeline we\u2019re building. The first two cells in the block below work together to fix some misspellings and ensure that we are using the same label for \u2018CarName\u2019 across all rows. The cell below is diagnostic. It turns out that we don\u2019t have any duplicates in this data set, but even if we did, we do not need to run this cell as part of the clean_data step or any pipeline step. We would only run cells that dealt with duplicates. This cell is just a means of getting a quick peek at the data. It doesn\u2019t do any data cleaning. !!! important \"Follow Along Before continuing, please ensure the clean_data step in your notebook matches the solution above.","title":"Solution"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-cells/","text":"Lab: Skip Cells Now, as a lab exercise, take a look at all the cells currently included in the clean_data step and update those cells to meet the requirements specified below. Requirements Update the cells in the Clean Data section of our notebook so that cells not necessary for cleaning data will be excluded from the clean_data pipeline step. Here\u2019s what clean_data looks like as you begin this exercise. Solution Once you\u2019ve made the updates you believe are required, take a look at the solution and verify that you\u2019ve made the right changes. Follow Along Before continuing, please ensure the clean_data step in your notebook matches the solution we've provided.","title":"Lab: Skip Cells"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-cells/#lab-skip-cells","text":"Now, as a lab exercise, take a look at all the cells currently included in the clean_data step and update those cells to meet the requirements specified below.","title":"Lab: Skip Cells"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-cells/#requirements","text":"Update the cells in the Clean Data section of our notebook so that cells not necessary for cleaning data will be excluded from the clean_data pipeline step. Here\u2019s what clean_data looks like as you begin this exercise.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-cells/#solution","text":"Once you\u2019ve made the updates you believe are required, take a look at the solution and verify that you\u2019ve made the right changes. Follow Along Before continuing, please ensure the clean_data step in your notebook matches the solution we've provided.","title":"Solution"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-section/","text":"Lab: Skip Section To keep the Visualize Data section in the notebook, but also have Kale ignore it, we need to annotate all the cells in this section as Skip Cells just like we did for the diagnostic cells in the two pipeline steps we\u2019ve already defined. Requirements Update your copy of our notebook so that all cells in the Visualize Data section of our notebook will be excluded from the Kubeflow pipeline we are building. Solution Expand to see solution When you\u2019re done, every cell in the Visualize Data section should be marked with the Skip Cell annotation. In total there should be six cells you\u2019ll need to annotate as a Skip Cell.","title":"Lab: Skip Section"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-section/#lab-skip-section","text":"To keep the Visualize Data section in the notebook, but also have Kale ignore it, we need to annotate all the cells in this section as Skip Cells just like we did for the diagnostic cells in the two pipeline steps we\u2019ve already defined.","title":"Lab: Skip Section"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-section/#requirements","text":"Update your copy of our notebook so that all cells in the Visualize Data section of our notebook will be excluded from the Kubeflow pipeline we are building.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/exclude/lab-skip-section/#solution","text":"Expand to see solution When you\u2019re done, every cell in the Visualize Data section should be marked with the Skip Cell annotation. In total there should be six cells you\u2019ll need to annotate as a Skip Cell.","title":"Solution"},{"location":"modules/notebook-to-pipeline/exclude/skip-cells/","text":"Skip Cells Not Used in a Step In their current form, our read_data and clean_data steps both contain a number of lines of code that generate diagnostic output or do other work that is not core to the work of those steps. For pipeline runs, we don\u2019t want to include this code, because it will add unnecessary compute cycles and execution time. We can keep this code in our notebook where it will be of use as we continue to develop our models, but explicitly exclude it from pipeline steps. To do this, we need to make sure that all the statements to be excluded are in separate cells from the code that is core to pipeline steps. Then, we can apply the Skip Cell annotation to these cells. Follow Along Please follow along in your own copy of our notebook as we annotate skip cells. For the read_data step, we\u2019ll want to skip the calls to df.auto.head() and df.auto.describe() . To do this, edit each cell and select Skip Cell from the Cell type pull down menu. When you\u2019ve done this successfully, your read_data step will look something like the figure below.","title":"Skip Cells Not Used in a Step"},{"location":"modules/notebook-to-pipeline/exclude/skip-cells/#skip-cells-not-used-in-a-step","text":"In their current form, our read_data and clean_data steps both contain a number of lines of code that generate diagnostic output or do other work that is not core to the work of those steps. For pipeline runs, we don\u2019t want to include this code, because it will add unnecessary compute cycles and execution time. We can keep this code in our notebook where it will be of use as we continue to develop our models, but explicitly exclude it from pipeline steps. To do this, we need to make sure that all the statements to be excluded are in separate cells from the code that is core to pipeline steps. Then, we can apply the Skip Cell annotation to these cells. Follow Along Please follow along in your own copy of our notebook as we annotate skip cells. For the read_data step, we\u2019ll want to skip the calls to df.auto.head() and df.auto.describe() . To do this, edit each cell and select Skip Cell from the Cell type pull down menu. When you\u2019ve done this successfully, your read_data step will look something like the figure below.","title":"Skip Cells Not Used in a Step"},{"location":"modules/notebook-to-pipeline/exclude/skip-sections/","text":"Skip Sections Not Used in Pipelines The Visualize Data section of this notebook informs how we build models. It is what we used to identify the significant independent variables in this dataset as reflected in this portion of the notebook. We use the outcome of our analysis to define the list sig_col near the top of the Modeling section. We don\u2019t need to run the cells in the Visualize Data section as part of our pipeline, though we should keep this section in the notebook because, as is the case with most projects of this nature, we will likely need to refine our models further and want to return to a visualization and analysis phase in a future iteration.","title":"Skip Sections Not Used"},{"location":"modules/notebook-to-pipeline/exclude/skip-sections/#skip-sections-not-used-in-pipelines","text":"The Visualize Data section of this notebook informs how we build models. It is what we used to identify the significant independent variables in this dataset as reflected in this portion of the notebook. We use the outcome of our analysis to define the list sig_col near the top of the Modeling section. We don\u2019t need to run the cells in the Visualize Data section as part of our pipeline, though we should keep this section in the notebook because, as is the case with most projects of this nature, we will likely need to refine our models further and want to return to a visualization and analysis phase in a future iteration.","title":"Skip Sections Not Used in Pipelines"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-prep_data-solution/","text":"Solution - Lab: Create prep_data Step Requirements To incorporate the Prep Data section as a step in our Kubeflow pipeline, please modify your copy of our notebook to meet the following requirements: Create a new pipeline step. Set the step name to prep_data . Specify the correct step on which prep_data depends as the Depends on parameter. As part of this annotation, include only cells that contain code that is core to this step. Exclude cells in the Prep Data section that are not core to the functionality of this step using the Skip Cell annotation. Solution Requirements 1, 2, 3, and 4: Apply the Pipeline Step annotation to the first cell in the Prep Data section to create a new pipeline step. Specify prep_data as the value for the Step name parameter. Specify clean_data as the step on which prep_data depends. We use clean_data here rather than read_data because we want the data in the df_auto data frame after it has been cleaned up by the operations in the clean_data step. These cells select just the significant columns we identified during data analysis. It is only these that we want to use in training our model, so these two cells are essential to the prep_data step. Our pipeline can now be depicted as: Requirement 5: This cell simply produces some diagnostic output used when we were developing this notebook to ensure that the data frame was being modified to include only the significant columns. We annotate it as a skip cell. Requirement 4: This cell transforms the categorical variables in our data set so that they can be used in training our model. It is essential to the prep_data step. Requirement 5: This cell produces diagnostic output. We annotate it as a skip cell.","title":"Solution - Lab: Create prep_data"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-prep_data-solution/#solution-lab-create-prep_data-step","text":"","title":"Solution - Lab: Create prep_data Step"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-prep_data-solution/#requirements","text":"To incorporate the Prep Data section as a step in our Kubeflow pipeline, please modify your copy of our notebook to meet the following requirements: Create a new pipeline step. Set the step name to prep_data . Specify the correct step on which prep_data depends as the Depends on parameter. As part of this annotation, include only cells that contain code that is core to this step. Exclude cells in the Prep Data section that are not core to the functionality of this step using the Skip Cell annotation.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-prep_data-solution/#solution","text":"Requirements 1, 2, 3, and 4: Apply the Pipeline Step annotation to the first cell in the Prep Data section to create a new pipeline step. Specify prep_data as the value for the Step name parameter. Specify clean_data as the step on which prep_data depends. We use clean_data here rather than read_data because we want the data in the df_auto data frame after it has been cleaned up by the operations in the clean_data step. These cells select just the significant columns we identified during data analysis. It is only these that we want to use in training our model, so these two cells are essential to the prep_data step. Our pipeline can now be depicted as: Requirement 5: This cell simply produces some diagnostic output used when we were developing this notebook to ensure that the data frame was being modified to include only the significant columns. We annotate it as a skip cell. Requirement 4: This cell transforms the categorical variables in our data set so that they can be used in training our model. It is essential to the prep_data step. Requirement 5: This cell produces diagnostic output. We annotate it as a skip cell.","title":"Solution"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-prep_data/","text":"Lab: Create prep_data step In the Prep Data section of our notebook, we are performing two operations: Selecting the significant independent variables (columns) that we will use as features for our regression models and only including these columns in our data frame. Encoding categorical variables so that they can be used by our models. Requirements To incorporate the Prep Data section as a step in our Kubeflow pipeline, please modify your copy of our notebook to meet the following requirements: Create a new pipeline step. Set the step name to prep_data . Specify the correct step on which prep_data depends as the Depends on parameter. As part of this annotation, include all cells that contain code that is core to the step prep_data . Exclude cells in the Prep Data section that are not core to the functionality of this step. Solution When you are finished, compare your notebook to the solution and make any necessary changes so that your notebook matches the solution.","title":"Lab: Create prep_data Step"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-prep_data/#lab-create-prep_data-step","text":"In the Prep Data section of our notebook, we are performing two operations: Selecting the significant independent variables (columns) that we will use as features for our regression models and only including these columns in our data frame. Encoding categorical variables so that they can be used by our models.","title":"Lab: Create prep_data step"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-prep_data/#requirements","text":"To incorporate the Prep Data section as a step in our Kubeflow pipeline, please modify your copy of our notebook to meet the following requirements: Create a new pipeline step. Set the step name to prep_data . Specify the correct step on which prep_data depends as the Depends on parameter. As part of this annotation, include all cells that contain code that is core to the step prep_data . Exclude cells in the Prep Data section that are not core to the functionality of this step.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-prep_data/#solution","text":"When you are finished, compare your notebook to the solution and make any necessary changes so that your notebook matches the solution.","title":"Solution"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-split_data-solution/","text":"Solution - Lab: Create split_data Step In the Build Models subsection of the Modeling section, we split data for training and evaluation and then build and evaluate three regression models. In this lab, we\u2019ll define a pipeline step for the data splitting portion of our workflow. Requirements Annotate one or more cells in the Build Models section of our notebook to meet the following requirements: Annotate one or more cells in our notebook to create a pipeline step named split_data that splits our dataset for use in later training and evaluation steps. Specify the correct dependency relationship for split_data . Solution Requirement 1: We accomplish splitting the data in just one cell. Annotate this cell as depicted below to create the split_data pipeline step. Requirement 2: The split_data step depends on the output of prep_data which selects just the significant columns in our dataset and transforms categorical columns for training. Our pipeline can now be depicted as:","title":"Solution - Lab: Create split_data"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-split_data-solution/#solution-lab-create-split_data-step","text":"In the Build Models subsection of the Modeling section, we split data for training and evaluation and then build and evaluate three regression models. In this lab, we\u2019ll define a pipeline step for the data splitting portion of our workflow.","title":"Solution - Lab: Create split_data Step"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-split_data-solution/#requirements","text":"Annotate one or more cells in the Build Models section of our notebook to meet the following requirements: Annotate one or more cells in our notebook to create a pipeline step named split_data that splits our dataset for use in later training and evaluation steps. Specify the correct dependency relationship for split_data .","title":"Requirements"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-split_data-solution/#solution","text":"Requirement 1: We accomplish splitting the data in just one cell. Annotate this cell as depicted below to create the split_data pipeline step. Requirement 2: The split_data step depends on the output of prep_data which selects just the significant columns in our dataset and transforms categorical columns for training. Our pipeline can now be depicted as:","title":"Solution"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-split_data/","text":"Lab: Create split_data Step In the Build Models subsection of the Modeling section, we split data for training and evaluation and then build and evaluate three regression models. In this lab, we\u2019ll define a pipeline step for the data splitting portion of our workflow. Requirements Annotate one or more cells in the Build Models section of our notebook to meet the following requirements: Annotate one or more cells in our notebook to create a pipeline step named split_data that splits our dataset for use in later training and evaluation steps. Specify the correct dependency relationship for split_data . Solution When you are finished, compare your notebook to the solution and make any necessary changes so that your notebook matches the solution.","title":"Lab: Create split_data Step"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-split_data/#lab-create-split_data-step","text":"In the Build Models subsection of the Modeling section, we split data for training and evaluation and then build and evaluate three regression models. In this lab, we\u2019ll define a pipeline step for the data splitting portion of our workflow.","title":"Lab: Create split_data Step"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-split_data/#requirements","text":"Annotate one or more cells in the Build Models section of our notebook to meet the following requirements: Annotate one or more cells in our notebook to create a pipeline step named split_data that splits our dataset for use in later training and evaluation steps. Specify the correct dependency relationship for split_data .","title":"Requirements"},{"location":"modules/notebook-to-pipeline/fundamentals-labs/lab-split_data/#solution","text":"When you are finished, compare your notebook to the solution and make any necessary changes so that your notebook matches the solution.","title":"Solution"},{"location":"modules/notebook-to-pipeline/imports/example/","text":"Apply the Imports Annotation We\u2019ll do a bit more reorganization in the Modeling section later in this module, so for now, let\u2019s just focus on the portion of our notebook up to the Modeling section. Follow Along Please follow along in your own copy of our notebook as we address these imports. Besides the first cell in the notebook, let\u2019s review where else we have imports in the cells leading up to the Modeling section. There are two cells in the Visualize Data section that include imports. They are as follows. Let's move these imports so that they are together with the others in the first cell of our notebook. Noce we've done that, the first cell in our notebook will look something like this. The modified cells in the Visualize Data section should look something like this. Now, let\u2019s apply the Imports annotation to the cell containing our reorganized imports.","title":"Apply the Imports Annotation"},{"location":"modules/notebook-to-pipeline/imports/example/#apply-the-imports-annotation","text":"We\u2019ll do a bit more reorganization in the Modeling section later in this module, so for now, let\u2019s just focus on the portion of our notebook up to the Modeling section. Follow Along Please follow along in your own copy of our notebook as we address these imports. Besides the first cell in the notebook, let\u2019s review where else we have imports in the cells leading up to the Modeling section. There are two cells in the Visualize Data section that include imports. They are as follows. Let's move these imports so that they are together with the others in the first cell of our notebook. Noce we've done that, the first cell in our notebook will look something like this. The modified cells in the Visualize Data section should look something like this. Now, let\u2019s apply the Imports annotation to the cell containing our reorganized imports.","title":"Apply the Imports Annotation"},{"location":"modules/notebook-to-pipeline/imports/lab-imports-solution/","text":"Solution - Lab: Imports Requirements Review all cells in the Modeling section of our notebook and make the following changes: Create a cell just below the \u201cModeling\u201d header. Move all the import statements below this point in the notebook into the new cell you\u2019ve just created. Annotate the new cell you created as an Imports cell. Solution In the Modeling section of our notebook, there are import statements in the Prep Data and Build Models subsections. In the Prep Data subsection, we have this cell. In the Build Models subsection, we have these cells. To satisfy requirements 1 and 2 , relocate the import statements from these three cells to a new cell just below the \u201cModeling\u201d header. When you\u2019re done: The new cell should look like the cell depicted below. The three cells depicted above should not contain any import statements. The third cell above should be removed since it will be empty after relocating the import statements it contains. To satisfy requirement 3 , apply the Imports annotation to our new cell.","title":"Solution - Lab: Imports"},{"location":"modules/notebook-to-pipeline/imports/lab-imports-solution/#solution-lab-imports","text":"","title":"Solution - Lab: Imports"},{"location":"modules/notebook-to-pipeline/imports/lab-imports-solution/#requirements","text":"Review all cells in the Modeling section of our notebook and make the following changes: Create a cell just below the \u201cModeling\u201d header. Move all the import statements below this point in the notebook into the new cell you\u2019ve just created. Annotate the new cell you created as an Imports cell.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/imports/lab-imports-solution/#solution","text":"In the Modeling section of our notebook, there are import statements in the Prep Data and Build Models subsections. In the Prep Data subsection, we have this cell. In the Build Models subsection, we have these cells. To satisfy requirements 1 and 2 , relocate the import statements from these three cells to a new cell just below the \u201cModeling\u201d header. When you\u2019re done: The new cell should look like the cell depicted below. The three cells depicted above should not contain any import statements. The third cell above should be removed since it will be empty after relocating the import statements it contains. To satisfy requirement 3 , apply the Imports annotation to our new cell.","title":"Solution"},{"location":"modules/notebook-to-pipeline/imports/lab-imports/","text":"Lab: Imports Organize and annotate a second Imports cell in your copy of our notebook. This cell should include all import statements currently included in cells found in the Modeling section of our notebook. Requirements Review all cells in the Modeling section of our notebook and make the following changes: Create a cell just below the \u201cModeling\u201d header. Move all the import statements below this point in the notebook into the new cell you\u2019ve just created. Annotate the new cell you created as an Imports cell. Solution When you are finished, compare your notebook to the solution and make any necessary changes so that your notebook matches the solution.","title":"Lab: Imports"},{"location":"modules/notebook-to-pipeline/imports/lab-imports/#lab-imports","text":"Organize and annotate a second Imports cell in your copy of our notebook. This cell should include all import statements currently included in cells found in the Modeling section of our notebook.","title":"Lab: Imports"},{"location":"modules/notebook-to-pipeline/imports/lab-imports/#requirements","text":"Review all cells in the Modeling section of our notebook and make the following changes: Create a cell just below the \u201cModeling\u201d header. Move all the import statements below this point in the notebook into the new cell you\u2019ve just created. Annotate the new cell you created as an Imports cell.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/imports/lab-imports/#solution","text":"When you are finished, compare your notebook to the solution and make any necessary changes so that your notebook matches the solution.","title":"Solution"},{"location":"modules/notebook-to-pipeline/imports/why-organize-imports/","text":"Gather Imports in One Cell Why? In general, though convenient, it\u2019s usually not a good idea to scatter imports throughout your code. It almost always leads to errors caused by references to modules that you end up using before the location where you had imported them originally. When building pipelines with Kale, it\u2019s essential that you organize your imports together in one or two cells and then annotate those cells using the Imports label. This will avoid pipeline execution errors due to missing modules and enable Kale to configure the execution environment for each pipeline step correctly. How Kale uses Imports cells Kale prepends the code in every cell annotated with Imports to the code in the cells you\u2019ve annotated with Pipeline Step . Kale executes each pipeline step as if it were a notebook composed of all Imports cells in your notebook followed by the cells annotated with Pipeline Step for the specific step in question. Therefore all imports, variable declarations and setup statements included in each Imports cell will execute before the core code for the pipeline step. Organizing imports together and annotating those cells as Imports ensures that Kale can organize the code for each step and set up its execution environment correctly.","title":"Gather Imports in One Cell"},{"location":"modules/notebook-to-pipeline/imports/why-organize-imports/#gather-imports-in-one-cell","text":"","title":"Gather Imports in One Cell"},{"location":"modules/notebook-to-pipeline/imports/why-organize-imports/#why","text":"In general, though convenient, it\u2019s usually not a good idea to scatter imports throughout your code. It almost always leads to errors caused by references to modules that you end up using before the location where you had imported them originally. When building pipelines with Kale, it\u2019s essential that you organize your imports together in one or two cells and then annotate those cells using the Imports label. This will avoid pipeline execution errors due to missing modules and enable Kale to configure the execution environment for each pipeline step correctly.","title":"Why?"},{"location":"modules/notebook-to-pipeline/imports/why-organize-imports/#how-kale-uses-imports-cells","text":"Kale prepends the code in every cell annotated with Imports to the code in the cells you\u2019ve annotated with Pipeline Step . Kale executes each pipeline step as if it were a notebook composed of all Imports cells in your notebook followed by the cells annotated with Pipeline Step for the specific step in question. Therefore all imports, variable declarations and setup statements included in each Imports cell will execute before the core code for the pipeline step. Organizing imports together and annotating those cells as Imports ensures that Kale can organize the code for each step and set up its execution environment correctly.","title":"How Kale uses Imports cells"},{"location":"modules/notebook-to-pipeline/overview/03-objective/","text":"Our Objective for this Module Though we believe the data science solution in our working example is reasonable, for the rest of this module we will not focus on the implementation. Instead, we will focus on a realistic example of the considerations you will want to make and the process you will need to follow to transform an existing notebook written in Python into a Kubeflow pipeline that will run on a Kubernetes cluster.","title":"Our Objective for this Module"},{"location":"modules/notebook-to-pipeline/overview/03-objective/#our-objective-for-this-module","text":"Though we believe the data science solution in our working example is reasonable, for the rest of this module we will not focus on the implementation. Instead, we will focus on a realistic example of the considerations you will want to make and the process you will need to follow to transform an existing notebook written in Python into a Kubeflow pipeline that will run on a Kubernetes cluster.","title":"Our Objective for this Module"},{"location":"modules/notebook-to-pipeline/pipeline-design/checkpoint/","text":"Checkpoint: Pipeline Design Having completed the lessons in this module, you should now be able to do the following comfortably: Identify code in a notebook that implements a discrete step in a machine learning workflow and annotate that cell as a Pipeline Step . Identify the data that one step produces as output and the step or steps that depend on that data as input. Specify single-step and multi-step dependency relationships between Kubeflow pipeline steps using the Depends on parameter of the Pipeline Step annotation. Create pipeline branches that can run in parallel using the Depends on parameter of the Pipeline Step annotation. Organize the Python statements that import modules your pipeline steps need into a small number of cells and mark those cells using the Imports annotation. Identify cells in a notebook that should be excluded from pipeline runs and annotate them as Skip Cells. Organize and annotate Functions cells.","title":"Checkpoint: Pipeline Design"},{"location":"modules/notebook-to-pipeline/pipeline-design/checkpoint/#checkpoint-pipeline-design","text":"Having completed the lessons in this module, you should now be able to do the following comfortably: Identify code in a notebook that implements a discrete step in a machine learning workflow and annotate that cell as a Pipeline Step . Identify the data that one step produces as output and the step or steps that depend on that data as input. Specify single-step and multi-step dependency relationships between Kubeflow pipeline steps using the Depends on parameter of the Pipeline Step annotation. Create pipeline branches that can run in parallel using the Depends on parameter of the Pipeline Step annotation. Organize the Python statements that import modules your pipeline steps need into a small number of cells and mark those cells using the Imports annotation. Identify cells in a notebook that should be excluded from pipeline runs and annotate them as Skip Cells. Organize and annotate Functions cells.","title":"Checkpoint: Pipeline Design"},{"location":"modules/notebook-to-pipeline/pipeline-design/example/","text":"Apply this Design Principle As an example of designing pipelines for iteration, we\u2019ll demonstrate reorganizing the code for the LGBM regression model into separate cells and steps. Follow Along Please follow along and make the corresponding changes in your own copy of our notebook. The code for the LGBM model is depicted in the figure above. As a first step, let\u2019s split this cell into multiple cells for model training, diagnostic output, and evaluation. We\u2019ll leave the print statements together with the evaluation code, because we want to output the result as part of the last step for these branches of our pipeline. Next, we\u2019ll annotate these cells to create two new pipeline steps, train_lgbm and eval_lgbm . split_data is the step on which train_lgbm depends and train_lgbm is the step on which eval_lgbm depends. Our pipeline can now be depicted as:","title":"Apply this Design Principle"},{"location":"modules/notebook-to-pipeline/pipeline-design/example/#apply-this-design-principle","text":"As an example of designing pipelines for iteration, we\u2019ll demonstrate reorganizing the code for the LGBM regression model into separate cells and steps. Follow Along Please follow along and make the corresponding changes in your own copy of our notebook. The code for the LGBM model is depicted in the figure above. As a first step, let\u2019s split this cell into multiple cells for model training, diagnostic output, and evaluation. We\u2019ll leave the print statements together with the evaluation code, because we want to output the result as part of the last step for these branches of our pipeline. Next, we\u2019ll annotate these cells to create two new pipeline steps, train_lgbm and eval_lgbm . split_data is the step on which train_lgbm depends and train_lgbm is the step on which eval_lgbm depends. Our pipeline can now be depicted as:","title":"Apply this Design Principle"},{"location":"modules/notebook-to-pipeline/pipeline-design/functions.cells/","text":"Functions Cells One annotation we\u2019ve not yet discussed is the Functions cell type. Kale provides this cell type to enable you to identify blocks of code containing: Functions used later in your machine learning pipeline. Global variable definitions (other than pipeline parameters) and code that initializes lists, dictionaries, objects, and other values used throughout your pipeline. Functions cells help Kale identify all dependencies for pipeline steps. As you know, Kale prepends the code in every cell annotated with Imports to the code in the cells you\u2019ve annotated with Pipeline Step . In addition, Kale also prepends the code in every Functions cell as part of this process. Kale executes each pipeline step as if it were a notebook composed of all Imports cells in your notebook followed by all Functions cells, and then followed by the cells annotated with Pipeline Step for the specific step in question. Therefore all imports, variable declarations and setup statements included in each Imports cell and all functions and other declarations found in all Functions cells will execute before the core code for the pipeline step. Organizing functions together and annotating those cells with the Functions label ensures that Kale can organize the code for each step and set up its execution environment correctly. Follow Along Please follow along and make the corresponding changes in your own copy of our notebook. Let\u2019s get some experience with Functions cells by abstracting out the scoring and output functionality from our evaluation steps. The last several lines for each of our evaluation steps are nearly identical. Let\u2019s write two functions to handle this logic and place them in a cell we\u2019ll annotate using the Functions label. We\u2019ll place this cell at the very top of the Build Models subsection Then let's modify each of our evaluation steps accordingly. This simple example illustrates the purpose of Functions cells and how to create and annotate them. In later modules, we\u2019ll make greater use of Functions cells.","title":"Functions Cells"},{"location":"modules/notebook-to-pipeline/pipeline-design/functions.cells/#functions-cells","text":"One annotation we\u2019ve not yet discussed is the Functions cell type. Kale provides this cell type to enable you to identify blocks of code containing: Functions used later in your machine learning pipeline. Global variable definitions (other than pipeline parameters) and code that initializes lists, dictionaries, objects, and other values used throughout your pipeline. Functions cells help Kale identify all dependencies for pipeline steps. As you know, Kale prepends the code in every cell annotated with Imports to the code in the cells you\u2019ve annotated with Pipeline Step . In addition, Kale also prepends the code in every Functions cell as part of this process. Kale executes each pipeline step as if it were a notebook composed of all Imports cells in your notebook followed by all Functions cells, and then followed by the cells annotated with Pipeline Step for the specific step in question. Therefore all imports, variable declarations and setup statements included in each Imports cell and all functions and other declarations found in all Functions cells will execute before the core code for the pipeline step. Organizing functions together and annotating those cells with the Functions label ensures that Kale can organize the code for each step and set up its execution environment correctly. Follow Along Please follow along and make the corresponding changes in your own copy of our notebook. Let\u2019s get some experience with Functions cells by abstracting out the scoring and output functionality from our evaluation steps. The last several lines for each of our evaluation steps are nearly identical. Let\u2019s write two functions to handle this logic and place them in a cell we\u2019ll annotate using the Functions label. We\u2019ll place this cell at the very top of the Build Models subsection Then let's modify each of our evaluation steps accordingly. This simple example illustrates the purpose of Functions cells and how to create and annotate them. In later modules, we\u2019ll make greater use of Functions cells.","title":"Functions Cells"},{"location":"modules/notebook-to-pipeline/pipeline-design/iteration/","text":"Design for Iteration To complete our pipeline we need to do a little code reorganization. We\u2019ll be training and evaluating three models simultaneously. It doesn\u2019t make sense to combine the model training and evaluation code in a single cell or step as we have it in the example depicted below. We can use the resources of a Kubernetes cluster more efficiently if we split these phases into separate pipeline steps. In addition, Kale provides a snapshotting feature that enables you to return to the execution state of any step during a pipeline run. So, if you want to make changes to an evaluation step, you can do so and then rerun the pipeline from just after the training step completes. For long-running pipelines this can save a lot of time. Important Break your pipeline down to separate all discrete steps you might want to iterate on independently from other components of your workflow. As an example of designing pipelines for iteration, we\u2019ll demonstrate reorganizing the code for the LGBM regression model into separate cells and steps. Follow Along Please follow along and make the corresponding changes in your own copy of our notebook. The code for the LGBM model is depicted in the figure above. As a first step, let\u2019s split this cell into multiple cells for model training, diagnostic output, and evaluation. We\u2019ll leave the print statements together with the evaluation code, because we want to output the result as part of the last step for these branches of our pipeline. Next, we\u2019ll annotate these cells to create two new pipeline steps, train_lgbm and eval_lgbm . split_data is the step on which train_lgbm depends and train_lgbm is the step on which eval_lgbm depends. Our pipeline can now be depicted as:","title":"Design for Iteration"},{"location":"modules/notebook-to-pipeline/pipeline-design/iteration/#design-for-iteration","text":"To complete our pipeline we need to do a little code reorganization. We\u2019ll be training and evaluating three models simultaneously. It doesn\u2019t make sense to combine the model training and evaluation code in a single cell or step as we have it in the example depicted below. We can use the resources of a Kubernetes cluster more efficiently if we split these phases into separate pipeline steps. In addition, Kale provides a snapshotting feature that enables you to return to the execution state of any step during a pipeline run. So, if you want to make changes to an evaluation step, you can do so and then rerun the pipeline from just after the training step completes. For long-running pipelines this can save a lot of time. Important Break your pipeline down to separate all discrete steps you might want to iterate on independently from other components of your workflow. As an example of designing pipelines for iteration, we\u2019ll demonstrate reorganizing the code for the LGBM regression model into separate cells and steps. Follow Along Please follow along and make the corresponding changes in your own copy of our notebook. The code for the LGBM model is depicted in the figure above. As a first step, let\u2019s split this cell into multiple cells for model training, diagnostic output, and evaluation. We\u2019ll leave the print statements together with the evaluation code, because we want to output the result as part of the last step for these branches of our pipeline. Next, we\u2019ll annotate these cells to create two new pipeline steps, train_lgbm and eval_lgbm . split_data is the step on which train_lgbm depends and train_lgbm is the step on which eval_lgbm depends. Our pipeline can now be depicted as:","title":"Design for Iteration"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-rf-train-eval-solution/","text":"Solution - Lab: RF train and eval steps Following a process similar to what we did for the LGBM regression model in the previous section, reorganize the code and apply the appropriate annotations for the RandomForest (RF) model. For this lab, the code you will work with is found in this cell. Requirements Reorganize and annotate the code for the RF model to meet the following requirements: Create a new pipeline step called train_rf to train the RF model. Create a new pipeline step called eval_rf to evaluate the RF model. Specify the correct dependency relationships for both steps. Note that the train_rf step begins a branch in our pipeline. This branch can run in parallel with the branch for the LGBM model. For each step, include only cells that contain code that is core to the step. Exclude cells that are not core to one step or the other using the Skip Cell annotation. Solution Requirements 1, 4 : Following the example for the LGBM model, we need to isolate the code that creates and trains the RandomForest model in a single cell. Requirements 1 and 4 are addressed in the code depicted in this figure. Requirement 3 : Like, train_lgbm , this step has split_data as a dependency. This creates a branch in our pipeline. Requirement 5 : This cell just prints a diagnostic message. We can skip it in pipeline execution. Requirements 2, 4 : The code for the eval_rf step is depicted in the figure below. We\u2019ll leave the print statements together with the evaluation code, because we want to output the result as part of the last step for this branch of our pipeline. Requirement 3 : This step depends on train_rf (uses the value of rf_model ) and indirectly on split_data because it uses the data values referenced by the variables: x , y , xt , and yt .","title":"Solution - Lab: RF train and eval"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-rf-train-eval-solution/#solution-lab-rf-train-and-eval-steps","text":"Following a process similar to what we did for the LGBM regression model in the previous section, reorganize the code and apply the appropriate annotations for the RandomForest (RF) model. For this lab, the code you will work with is found in this cell.","title":"Solution - Lab: RF train and eval steps"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-rf-train-eval-solution/#requirements","text":"Reorganize and annotate the code for the RF model to meet the following requirements: Create a new pipeline step called train_rf to train the RF model. Create a new pipeline step called eval_rf to evaluate the RF model. Specify the correct dependency relationships for both steps. Note that the train_rf step begins a branch in our pipeline. This branch can run in parallel with the branch for the LGBM model. For each step, include only cells that contain code that is core to the step. Exclude cells that are not core to one step or the other using the Skip Cell annotation.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-rf-train-eval-solution/#solution","text":"Requirements 1, 4 : Following the example for the LGBM model, we need to isolate the code that creates and trains the RandomForest model in a single cell. Requirements 1 and 4 are addressed in the code depicted in this figure. Requirement 3 : Like, train_lgbm , this step has split_data as a dependency. This creates a branch in our pipeline. Requirement 5 : This cell just prints a diagnostic message. We can skip it in pipeline execution. Requirements 2, 4 : The code for the eval_rf step is depicted in the figure below. We\u2019ll leave the print statements together with the evaluation code, because we want to output the result as part of the last step for this branch of our pipeline. Requirement 3 : This step depends on train_rf (uses the value of rf_model ) and indirectly on split_data because it uses the data values referenced by the variables: x , y , xt , and yt .","title":"Solution"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-rf-train-eval/","text":"Lab: Create a train_rf and eval_rf steps Following a process similar to what we did for the LGBM regression model in the previous section, reorganize the code and apply the appropriate annotations for the RandomForest (RF) model. For this lab, the code you will work with is found in this cell. Requirements Reorganize and annotate the code for the RF model to meet the following requirements: Create a new pipeline step called train_rf to train the RF model. Create a new pipeline step called eval_rf to evaluate the RF model. Specify the correct dependency relationships for both steps. Note that the train_rf step begins a branch in our pipeline. This branch can run in parallel with the branch for the LGBM model. For each step, include only cells that contain code that is core to the step. Exclude cells that are not core to one step or the other using the Skip Cell annotation. Solution When you are finished, compare your notebook to the solution and make any necessary changes so that your notebook matches the solution.","title":"Lab: RF train and eval steps"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-rf-train-eval/#lab-create-a-train_rf-and-eval_rf-steps","text":"Following a process similar to what we did for the LGBM regression model in the previous section, reorganize the code and apply the appropriate annotations for the RandomForest (RF) model. For this lab, the code you will work with is found in this cell.","title":"Lab: Create a train_rf and eval_rf steps"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-rf-train-eval/#requirements","text":"Reorganize and annotate the code for the RF model to meet the following requirements: Create a new pipeline step called train_rf to train the RF model. Create a new pipeline step called eval_rf to evaluate the RF model. Specify the correct dependency relationships for both steps. Note that the train_rf step begins a branch in our pipeline. This branch can run in parallel with the branch for the LGBM model. For each step, include only cells that contain code that is core to the step. Exclude cells that are not core to one step or the other using the Skip Cell annotation.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-rf-train-eval/#solution","text":"When you are finished, compare your notebook to the solution and make any necessary changes so that your notebook matches the solution.","title":"Solution"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-xgb-train-eval-solution/","text":"Solution - Lab: XGB train and eval steps Following a process similar to what we did for the LGBM and RF regression models above, reorganize the code and apply the appropriate annotations for the XGB model. For this lab, the code you will work with is found in this cell. Requirements Reorganize and annotate the code for the XGB model to meet the following requirements: Create a new pipeline step called train_xgb to train the XGB model. Create a new pipeline step called eval_xgb to evaluate the XGB model. Specify the correct dependency relationships for both steps. Note that the train_xgb step begins a branch in our pipeline. This branch can run in parallel with the branches for the LGBM and RF models. For each step, include only cells that contain code that is core to the step. Exclude cells that are not core to one step or the other using the Skip Cell annotation. Solution Requirements 1, 4 : Following the example for the XGB model, we need to isolate the code that creates and trains the XGB model in a single cell. Requirements 1 and 4 are addressed in the code depicted in this figure. Requirement 3 : Like, train_lgbm and train_xgb , this step has split_data as a dependency. Requirement 5 : There are no cells to skip for this lab. Requirements 2, 4 : The code for the eval_xgb step is depicted in the figure below. We\u2019ll leave the print statements together with the evaluation code, because we do want to output the result as part of the last step for this branch of our pipeline. Requirement 3 : This step depends on train_xgb (uses the value of xgb_grid ) and indirectly on split_data because it uses the data values referenced by the variables: xt and yt .","title":"Solution - Lab: XGB train and eval"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-xgb-train-eval-solution/#solution-lab-xgb-train-and-eval-steps","text":"Following a process similar to what we did for the LGBM and RF regression models above, reorganize the code and apply the appropriate annotations for the XGB model. For this lab, the code you will work with is found in this cell.","title":"Solution - Lab: XGB train and eval steps"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-xgb-train-eval-solution/#requirements","text":"Reorganize and annotate the code for the XGB model to meet the following requirements: Create a new pipeline step called train_xgb to train the XGB model. Create a new pipeline step called eval_xgb to evaluate the XGB model. Specify the correct dependency relationships for both steps. Note that the train_xgb step begins a branch in our pipeline. This branch can run in parallel with the branches for the LGBM and RF models. For each step, include only cells that contain code that is core to the step. Exclude cells that are not core to one step or the other using the Skip Cell annotation.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-xgb-train-eval-solution/#solution","text":"Requirements 1, 4 : Following the example for the XGB model, we need to isolate the code that creates and trains the XGB model in a single cell. Requirements 1 and 4 are addressed in the code depicted in this figure. Requirement 3 : Like, train_lgbm and train_xgb , this step has split_data as a dependency. Requirement 5 : There are no cells to skip for this lab. Requirements 2, 4 : The code for the eval_xgb step is depicted in the figure below. We\u2019ll leave the print statements together with the evaluation code, because we do want to output the result as part of the last step for this branch of our pipeline. Requirement 3 : This step depends on train_xgb (uses the value of xgb_grid ) and indirectly on split_data because it uses the data values referenced by the variables: xt and yt .","title":"Solution"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-xgb-train-eval/","text":"Lab: XGB train and eval steps Following a process similar to what we did for the LGBM and RF regression models above, reorganize the code and apply the appropriate annotations for the XGB model. For this lab, the code you will work with is found in this cell. Requirements Reorganize and annotate the code for the XGB model to meet the following requirements: Create a new pipeline step called train_xgb to train the XGB model. Create a new pipeline step called eval_xgb to evaluate the XGB model. Specify the correct dependency relationships for both steps. Note that the train_xgb step begins a branch in our pipeline. This branch can run in parallel with the branches for the LGBM and RF models. For each step, include only cells that contain code that is core to the step. Exclude cells that are not core to one step or the other using the Skip Cell annotation. Solution When you are finished, compare your notebook to the solution and make any necessary changes so that your notebook matches the solution.","title":"Lab: XGB train and eval steps"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-xgb-train-eval/#lab-xgb-train-and-eval-steps","text":"Following a process similar to what we did for the LGBM and RF regression models above, reorganize the code and apply the appropriate annotations for the XGB model. For this lab, the code you will work with is found in this cell.","title":"Lab: XGB train and eval steps"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-xgb-train-eval/#requirements","text":"Reorganize and annotate the code for the XGB model to meet the following requirements: Create a new pipeline step called train_xgb to train the XGB model. Create a new pipeline step called eval_xgb to evaluate the XGB model. Specify the correct dependency relationships for both steps. Note that the train_xgb step begins a branch in our pipeline. This branch can run in parallel with the branches for the LGBM and RF models. For each step, include only cells that contain code that is core to the step. Exclude cells that are not core to one step or the other using the Skip Cell annotation.","title":"Requirements"},{"location":"modules/notebook-to-pipeline/pipeline-design/lab-xgb-train-eval/#solution","text":"When you are finished, compare your notebook to the solution and make any necessary changes so that your notebook matches the solution.","title":"Solution"},{"location":"modules/notebook-to-pipeline/pipeline-design/run-our-pipeline/","text":"Run Our Pipeline Now it\u2019s time to run our pipeline. To do this: Open the Kale Deployment panel, by clicking on the Kale icon in your Jupyter notebook environment. Click the pull-down menu for Select experiment and create a new experiment called car-price. Enter \u201cpredict-car-price\u201d as the pipeline name. Click the COMPILE AND RUN button. Once the pipeline is running, view the run by clicking the View link. This will open a panel to enable you to view the complete pipeline graph as the pipeline executes. Note that, as expected, training and evaluation for our models run in parallel. The steps in the pipeline are clickable and provide detailed information about that step. Most of the detail view for a step is outside the scope of this module, but let\u2019s click on the output step and view the Logs tab. If we zoom in, we can see the output produced by the output step reporting on the prediction performance of all three of our models. Feel free to explore other output tabs and other aspects of the pipeline run. We\u2019ll address everything you see here in upcoming training modules.","title":"Run Our Pipeline"},{"location":"modules/notebook-to-pipeline/pipeline-design/run-our-pipeline/#run-our-pipeline","text":"Now it\u2019s time to run our pipeline. To do this: Open the Kale Deployment panel, by clicking on the Kale icon in your Jupyter notebook environment. Click the pull-down menu for Select experiment and create a new experiment called car-price. Enter \u201cpredict-car-price\u201d as the pipeline name. Click the COMPILE AND RUN button. Once the pipeline is running, view the run by clicking the View link. This will open a panel to enable you to view the complete pipeline graph as the pipeline executes. Note that, as expected, training and evaluation for our models run in parallel. The steps in the pipeline are clickable and provide detailed information about that step. Most of the detail view for a step is outside the scope of this module, but let\u2019s click on the output step and view the Logs tab. If we zoom in, we can see the output produced by the output step reporting on the prediction performance of all three of our models. Feel free to explore other output tabs and other aspects of the pipeline run. We\u2019ll address everything you see here in upcoming training modules.","title":"Run Our Pipeline"},{"location":"modules/notebook-to-pipeline/pipeline-design/steps-with-multiple-dependencies/","text":"Steps w/ Multiple Dependencies In many cases, you\u2019ll need to create a step that depends on two or more other steps. Steps for producing output are one example. Let\u2019s take a look at how to set multiple dependencies by creating a final output step in our notebook. Follow Along Please follow along and make the corresponding changes in your own copy of our notebook. With the changes we\u2019ve just made to create a Functions cell containing functions for scoring and output to our notebook, each of our evaluation steps now calls output_results as its last statement. We can see this, for example in eval_xgb . Let\u2019s move the calls to output_results from each of our evaluation steps to a new step called output. Let\u2019s place the cell as the very last cell in our notebook. When we\u2019ve finished moving the three output_results statements to this cell, it should look like this. Now let\u2019s annotate this cell as a Pipeline Step and set the dependencies. Note that this step will depend on eval_lgbm , eval_rf , and eval_xgb because it uses the values for each score metric from each of these steps. The Depends on pull-down menu functions as a toggle. You may select as many dependencies as required. You may unselect options by clicking that item again in the pull-down menu. As depicted below, we\u2019ll select the three eval steps as dependencies for our new step, output . If you make a mistake in selecting one of the dependencies, click the incorrect dependency again in this pull-down and that item will be deselected. Once we\u2019ve selected all the dependencies, we can click away from the pull-down menu and you\u2019ll see that all three evaluation steps have been selected as dependencies.","title":"Steps w/ Multiple Dependencies"},{"location":"modules/notebook-to-pipeline/pipeline-design/steps-with-multiple-dependencies/#steps-w-multiple-dependencies","text":"In many cases, you\u2019ll need to create a step that depends on two or more other steps. Steps for producing output are one example. Let\u2019s take a look at how to set multiple dependencies by creating a final output step in our notebook. Follow Along Please follow along and make the corresponding changes in your own copy of our notebook. With the changes we\u2019ve just made to create a Functions cell containing functions for scoring and output to our notebook, each of our evaluation steps now calls output_results as its last statement. We can see this, for example in eval_xgb . Let\u2019s move the calls to output_results from each of our evaluation steps to a new step called output. Let\u2019s place the cell as the very last cell in our notebook. When we\u2019ve finished moving the three output_results statements to this cell, it should look like this. Now let\u2019s annotate this cell as a Pipeline Step and set the dependencies. Note that this step will depend on eval_lgbm , eval_rf , and eval_xgb because it uses the values for each score metric from each of these steps. The Depends on pull-down menu functions as a toggle. You may select as many dependencies as required. You may unselect options by clicking that item again in the pull-down menu. As depicted below, we\u2019ll select the three eval steps as dependencies for our new step, output . If you make a mistake in selecting one of the dependencies, click the incorrect dependency again in this pull-down and that item will be deselected. Once we\u2019ve selected all the dependencies, we can click away from the pull-down menu and you\u2019ll see that all three evaluation steps have been selected as dependencies.","title":"Steps w/ Multiple Dependencies"},{"location":"modules/notebook-to-pipeline/steps/dependencies/","text":"Dependencies Between Pipeline Steps In order to define a pipeline you need to identify not just the code that makes up the step, but also specify the order in which the steps of your pipeline should execute. To do this, select which step (or steps) should immediately precede the step you are annotating by using the Depends on pull-down menu. Follow Along Please follow along in your own copy of our notebook as we add a dependency for clean_data. The step clean_data relies on read_data to read our dataset into a pandas data frame ( df_auto ) so we need to define that relationship and establish the sequence in which these two steps should execute. With the work we\u2019ve done so far, we now have a two-step pipeline that we can summarize as follows. Make sure you understand your data dependencies When considering how to organize your notebook into a Kubeflow pipeline it is essential that you assess the data dependencies between steps and ensure that values you\u2019ve set for the Depends on field for each step reflect these dependencies.","title":"Pipeline Step Dependencies"},{"location":"modules/notebook-to-pipeline/steps/dependencies/#dependencies-between-pipeline-steps","text":"In order to define a pipeline you need to identify not just the code that makes up the step, but also specify the order in which the steps of your pipeline should execute. To do this, select which step (or steps) should immediately precede the step you are annotating by using the Depends on pull-down menu. Follow Along Please follow along in your own copy of our notebook as we add a dependency for clean_data. The step clean_data relies on read_data to read our dataset into a pandas data frame ( df_auto ) so we need to define that relationship and establish the sequence in which these two steps should execute. With the work we\u2019ve done so far, we now have a two-step pipeline that we can summarize as follows. Make sure you understand your data dependencies When considering how to organize your notebook into a Kubeflow pipeline it is essential that you assess the data dependencies between steps and ensure that values you\u2019ve set for the Depends on field for each step reflect these dependencies.","title":"Dependencies Between Pipeline Steps"},{"location":"modules/notebook-to-pipeline/steps/multi-cell/","text":"Multi-Cell Pipeline Steps Text Version of this Lesson The Clean Data section of our notebook performs three different tasks to clean up our dataset. There are several cells here that, together, do the work of a data cleaning step. As we hinted in the previous section, Kale enables you to include multiple cells in a single annotation for a pipeline step. Follow Along Please follow along in your own copy of our notebook as we complete the steps below. Let\u2019s define the second step of our pipeline. As we did before, we need to annotate a cell with the Pipeline Step label. In situations like this where the step is composed of multiple cells, you\u2019ll want to ensure that all cells are annotated accordingly. Annotate the first cell of the data cleaning step and name this step clean_data . The remaining cells for this step will be included in this annotation. If you can\u2019t remember exactly how to annotate a cell, see the example for read_data above or review the Kale documentation. When you have finished annotating clean_data , that portion of your notebook should look like the following.","title":"Multi-Cell Pipeline Steps"},{"location":"modules/notebook-to-pipeline/steps/multi-cell/#multi-cell-pipeline-steps","text":"","title":"Multi-Cell Pipeline Steps"},{"location":"modules/notebook-to-pipeline/steps/multi-cell/#text-version-of-this-lesson","text":"The Clean Data section of our notebook performs three different tasks to clean up our dataset. There are several cells here that, together, do the work of a data cleaning step. As we hinted in the previous section, Kale enables you to include multiple cells in a single annotation for a pipeline step. Follow Along Please follow along in your own copy of our notebook as we complete the steps below. Let\u2019s define the second step of our pipeline. As we did before, we need to annotate a cell with the Pipeline Step label. In situations like this where the step is composed of multiple cells, you\u2019ll want to ensure that all cells are annotated accordingly. Annotate the first cell of the data cleaning step and name this step clean_data . The remaining cells for this step will be included in this annotation. If you can\u2019t remember exactly how to annotate a cell, see the example for read_data above or review the Kale documentation. When you have finished annotating clean_data , that portion of your notebook should look like the following.","title":"Text Version of this Lesson"},{"location":"modules/notebook-to-pipeline/steps/single-cell/","text":"Single-Cell Pipeline Steps Text Version of this Lesson The first step of our workflow is reading data. That section of the notebook looks like this. This line of code reads the dataset into a pandas data frame. When working with Kubeflow pipelines, it\u2019s important to be specific about exactly what code implements a pipeline step and to consider the modules on which the code depends. We also need to consider the data inputs and outputs for a given step. This step depends on the pandas module. It does not have any inputs since it is the first step of our pipeline, but it does define and set the variable df_auto , which will be used in many later steps. For purposes of Kubeflow pipelines, df_auto is an output of this pipeline step. To create our first pipeline step, we\u2019ll do a small amount of reorganizing and then apply a few annotations. Follow Along Please follow along in your own copy of our notebook as we complete the steps below. Isolate the code for your step in one cell Modify your code so that the line that reads the car_prices.csv file is in a cell by itself. Annotate the cell as a Pipeline Step and name it Click the pencil icon on that cell and set the Cell type to Pipeline Step and the Step name to read_data . Click the x to close the annotation editor. Note that in addition to the label, read_data , this cell of our pipeline is now marked with a vertical line that is the same color as the background of the label, read_data . If you look more closely, you\u2019ll see that, in fact, all cells below this first cell have been marked with a vertical line of the same color. The default behavior for Kale is that it automatically includes the cells that follow a step cell as part of the same step until you specify otherwise by supplying annotations later in the notebook. In its current state, our entire notebook after the cell in which we read the car_prices.csv file is a single pipeline step. Obviously, we don\u2019t want the entire notebook to be a single-step pipeline, but this Kale behavior does provide an important convenience as we\u2019ll see in a moment.","title":"Single-Cell Pipeline Steps"},{"location":"modules/notebook-to-pipeline/steps/single-cell/#single-cell-pipeline-steps","text":"","title":"Single-Cell Pipeline Steps"},{"location":"modules/notebook-to-pipeline/steps/single-cell/#text-version-of-this-lesson","text":"The first step of our workflow is reading data. That section of the notebook looks like this. This line of code reads the dataset into a pandas data frame. When working with Kubeflow pipelines, it\u2019s important to be specific about exactly what code implements a pipeline step and to consider the modules on which the code depends. We also need to consider the data inputs and outputs for a given step. This step depends on the pandas module. It does not have any inputs since it is the first step of our pipeline, but it does define and set the variable df_auto , which will be used in many later steps. For purposes of Kubeflow pipelines, df_auto is an output of this pipeline step. To create our first pipeline step, we\u2019ll do a small amount of reorganizing and then apply a few annotations. Follow Along Please follow along in your own copy of our notebook as we complete the steps below.","title":"Text Version of this Lesson"},{"location":"modules/notebook-to-pipeline/steps/single-cell/#isolate-the-code-for-your-step-in-one-cell","text":"Modify your code so that the line that reads the car_prices.csv file is in a cell by itself.","title":"Isolate the code for your step in one cell"},{"location":"modules/notebook-to-pipeline/steps/single-cell/#annotate-the-cell-as-a-pipeline-step-and-name-it","text":"Click the pencil icon on that cell and set the Cell type to Pipeline Step and the Step name to read_data . Click the x to close the annotation editor. Note that in addition to the label, read_data , this cell of our pipeline is now marked with a vertical line that is the same color as the background of the label, read_data . If you look more closely, you\u2019ll see that, in fact, all cells below this first cell have been marked with a vertical line of the same color. The default behavior for Kale is that it automatically includes the cells that follow a step cell as part of the same step until you specify otherwise by supplying annotations later in the notebook. In its current state, our entire notebook after the cell in which we read the car_prices.csv file is a single pipeline step. Obviously, we don\u2019t want the entire notebook to be a single-step pipeline, but this Kale behavior does provide an important convenience as we\u2019ll see in a moment.","title":"Annotate the cell as a Pipeline Step and name it"}]}